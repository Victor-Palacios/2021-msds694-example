{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:22.123401Z",
     "start_time": "2021-02-12T19:36:19.118697Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "import math\n",
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create iris dataframe using the irisSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:22.129713Z",
     "start_time": "2021-02-12T19:36:22.126069Z"
    }
   },
   "outputs": [],
   "source": [
    "irisSchema = StructType([StructField(\"sepal_length\", DoubleType(), True), \n",
    "                         StructField(\"sepal_width\", DoubleType(), True),\n",
    "                         StructField(\"petal_length\", DoubleType(), True), \n",
    "                         StructField(\"petal_width\", DoubleType(), True),\n",
    "                         StructField(\"class\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:24.084910Z",
     "start_time": "2021-02-12T19:36:22.132175Z"
    }
   },
   "outputs": [],
   "source": [
    "iris = ss.read.csv('../Data/iris.csv', schema = irisSchema, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T23:55:12.620333Z",
     "start_time": "2021-02-11T23:55:07.499735Z"
    }
   },
   "source": [
    "## Change \"class\" to 1 if it is \"Iris-versicolor\", otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:24.089467Z",
     "start_time": "2021-02-12T19:36:24.086426Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = [\"Iris-versicolor\"]\n",
    "\n",
    "def check_class(x, codes):\n",
    "    if x in codes:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def check_class_udf(codes):\n",
    "    \"\"\"Create and register udf using check_class().\"\"\"\n",
    "    return udf(lambda x: check_class(x, classes), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:24.192835Z",
     "start_time": "2021-02-12T19:36:24.091401Z"
    }
   },
   "outputs": [],
   "source": [
    "iris_binary_class = iris.withColumn(\"class_int\", check_class_udf(classes)(\"class\"))\\\n",
    "                        .drop(\"class\")\\\n",
    "                        .withColumnRenamed(\"class_int\", \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:26.516138Z",
     "start_time": "2021-02-12T19:36:24.194325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|class|\n",
      "+------------+-----------+------------+-----------+-----+\n",
      "|         5.1|        3.5|         1.4|        0.2|    0|\n",
      "|         4.9|        3.0|         1.4|        0.2|    0|\n",
      "|         4.7|        3.2|         1.3|        0.2|    0|\n",
      "|         4.6|        3.1|         1.5|        0.2|    0|\n",
      "|         5.0|        3.6|         1.4|        0.2|    0|\n",
      "|         5.4|        3.9|         1.7|        0.4|    0|\n",
      "|         4.6|        3.4|         1.4|        0.3|    0|\n",
      "|         5.0|        3.4|         1.5|        0.2|    0|\n",
      "|         4.4|        2.9|         1.4|        0.2|    0|\n",
      "|         4.9|        3.1|         1.5|        0.1|    0|\n",
      "|         5.4|        3.7|         1.5|        0.2|    0|\n",
      "|         4.8|        3.4|         1.6|        0.2|    0|\n",
      "|         4.8|        3.0|         1.4|        0.1|    0|\n",
      "|         4.3|        3.0|         1.1|        0.1|    0|\n",
      "|         5.8|        4.0|         1.2|        0.2|    0|\n",
      "|         5.7|        4.4|         1.5|        0.4|    0|\n",
      "|         5.4|        3.9|         1.3|        0.4|    0|\n",
      "|         5.1|        3.5|         1.4|        0.3|    0|\n",
      "|         5.7|        3.8|         1.7|        0.3|    0|\n",
      "|         5.1|        3.8|         1.5|        0.3|    0|\n",
      "+------------+-----------+------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_binary_class.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:26.524641Z",
     "start_time": "2021-02-12T19:36:26.522521Z"
    }
   },
   "outputs": [],
   "source": [
    "input_cols = [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a feautre column combining\n",
    "## \"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\" and rename \"class\" to \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:26.822901Z",
     "start_time": "2021-02-12T19:36:26.526931Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=input_cols)\n",
    "\n",
    "#lpoints - labeled data.\n",
    "lpoints = va.transform(iris_binary_class)\\\n",
    ".select(\"features\", \"class\")\\\n",
    ".withColumnRenamed(\"class\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:27.039530Z",
     "start_time": "2021-02-12T19:36:26.824763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|features         |label|\n",
      "+-----------------+-----+\n",
      "|[5.1,3.5,1.4,0.2]|0    |\n",
      "|[4.9,3.0,1.4,0.2]|0    |\n",
      "|[4.7,3.2,1.3,0.2]|0    |\n",
      "|[4.6,3.1,1.5,0.2]|0    |\n",
      "|[5.0,3.6,1.4,0.2]|0    |\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert \"feature\" using MinMaxScaler\n",
    "https://spark.apache.org/docs/latest/ml-features.html#minmaxscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:27.592354Z",
     "start_time": "2021-02-12T19:36:27.041896Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def min_max_scaler(input_df):\n",
    "    df = input_df\n",
    "  \n",
    "    scaler = MinMaxScaler(inputCol=\"features\", \n",
    "                          outputCol=\"features_Scaled\")\n",
    "\n",
    "    mm = scaler.fit(df)\n",
    "\n",
    "    # Normalize each feature to have unit standard deviation.\n",
    "    df = mm.transform(df).drop(\"features\")\n",
    "    df = df.withColumnRenamed(\"features_Scaled\", \"features\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = min_max_scaler(lpoints).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:27.853753Z",
     "start_time": "2021-02-12T19:36:27.594157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------------------+\n",
      "|label|features                                                                          |\n",
      "+-----+----------------------------------------------------------------------------------+\n",
      "|0    |[0.22222222222222213,0.625,0.06779661016949151,0.04166666666666667]               |\n",
      "|0    |[0.1666666666666668,0.41666666666666663,0.06779661016949151,0.04166666666666667]  |\n",
      "|0    |[0.11111111111111119,0.5,0.05084745762711865,0.04166666666666667]                 |\n",
      "|0    |[0.08333333333333327,0.4583333333333333,0.0847457627118644,0.04166666666666667]   |\n",
      "|0    |[0.19444444444444448,0.6666666666666666,0.06779661016949151,0.04166666666666667]  |\n",
      "|0    |[0.30555555555555564,0.7916666666666665,0.11864406779661016,0.12500000000000003]  |\n",
      "|0    |[0.08333333333333327,0.5833333333333333,0.06779661016949151,0.08333333333333333]  |\n",
      "|0    |[0.19444444444444448,0.5833333333333333,0.0847457627118644,0.04166666666666667]   |\n",
      "|0    |[0.027777777777777922,0.37499999999999994,0.06779661016949151,0.04166666666666667]|\n",
      "|0    |[0.1666666666666668,0.4583333333333333,0.0847457627118644,0.0]                    |\n",
      "|0    |[0.30555555555555564,0.7083333333333334,0.0847457627118644,0.04166666666666667]   |\n",
      "|0    |[0.13888888888888887,0.5833333333333333,0.1016949152542373,0.04166666666666667]   |\n",
      "|0    |[0.13888888888888887,0.41666666666666663,0.06779661016949151,0.0]                 |\n",
      "|0    |[0.0,0.41666666666666663,0.016949152542372895,0.0]                                |\n",
      "|0    |[0.41666666666666663,0.8333333333333333,0.033898305084745756,0.04166666666666667] |\n",
      "|0    |[0.38888888888888895,1.0,0.0847457627118644,0.12500000000000003]                  |\n",
      "|0    |[0.30555555555555564,0.7916666666666665,0.05084745762711865,0.12500000000000003]  |\n",
      "|0    |[0.22222222222222213,0.625,0.06779661016949151,0.08333333333333333]               |\n",
      "|0    |[0.38888888888888895,0.7499999999999999,0.11864406779661016,0.08333333333333333]  |\n",
      "|0    |[0.22222222222222213,0.7499999999999999,0.0847457627118644,0.08333333333333333]   |\n",
      "+-----+----------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data set to training (80%)  and validation (20%) randomly, using seed value, 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:27.908626Z",
     "start_time": "2021-02-12T19:36:27.855921Z"
    }
   },
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8, 0.2], 1)\n",
    "\n",
    "#cache() : the algorithm is interative and training and data sets are going to be reused many times.\n",
    "train = splits[0].cache()\n",
    "valid = splits[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model, using the training data set with regParam=0.01, maxIter=10000, fitIntercept=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:29.439393Z",
     "start_time": "2021-02-12T19:36:27.910427Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(regParam=0.01, maxIter=10000, fitIntercept=True)\n",
    "lrmodel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the model to the validation set, and return areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:29.625224Z",
     "start_time": "2021-02-12T19:36:29.440876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|    0|[0.05555555555555...|[-1.2655224380680...|[0.22002470028658...|       1.0|\n",
      "|    0|[0.13888888888888...|[0.88335243716583...|[0.70751644644265...|       0.0|\n",
      "|    0|[0.13888888888888...|[1.78618666690854...|[0.85645911411034...|       0.0|\n",
      "|    0|[0.13888888888888...|[1.70525473495273...|[0.84621978807722...|       0.0|\n",
      "|    0|[0.16666666666666...|[0.60888545685682...|[0.6476865178204,...|       0.0|\n",
      "|    0|[0.22222222222222...|[1.66179572899768...|[0.84047891105382...|       0.0|\n",
      "|    0|[0.22222222222222...|[2.15397602005891...|[0.89603973684408...|       0.0|\n",
      "|    0|[0.22222222222222...|[2.82420875959698...|[0.94397008648642...|       0.0|\n",
      "|    0|[0.30555555555555...|[1.59434820606471...|[0.83122698602890...|       0.0|\n",
      "|    0|[0.30555555555555...|[3.39762229230004...|[0.96763014337496...|       0.0|\n",
      "|    0|[0.38888888888888...|[4.76596081213807...|[0.99155718478574...|       0.0|\n",
      "|    0|[0.41666666666666...|[0.85859615223918...|[0.70236726689244...|       0.0|\n",
      "|    0|[0.47222222222222...|[0.91454047179031...|[0.71392838442284...|       0.0|\n",
      "|    0|[0.55555555555555...|[-0.1366732128411...|[0.46588478505139...|       1.0|\n",
      "|    0|[0.61111111111111...|[0.85988806474571...|[0.70263726733163...|       0.0|\n",
      "|    0|[0.61111111111111...|[0.58831500540016...|[0.64297843636479...|       0.0|\n",
      "|    0|[0.69444444444444...|[0.79184612138881...|[0.68822759112399...|       0.0|\n",
      "|    0|[0.69444444444444...|[1.47646749266593...|[0.81403842332992...|       0.0|\n",
      "|    0|[0.77777777777777...|[0.60150630368511...|[0.64600084911152...|       0.0|\n",
      "|    0|[0.83333333333333...|[-0.1482630899854...|[0.46300197672573...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validpredicts = lrmodel.transform(valid)\n",
    "validpredicts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:29.915084Z",
     "start_time": "2021-02-12T19:36:29.627530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC:0.8166666666666667\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bceval = BinaryClassificationEvaluator()\n",
    "print (bceval.getMetricName() +\":\" + str(bceval.evaluate(validpredicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T19:36:30.786412Z",
     "start_time": "2021-02-12T19:36:29.916831Z"
    }
   },
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
